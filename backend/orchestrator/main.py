import os
import shutil
import logging
import json
import time
from backend.utils.models import AgentState
from backend.utils.paths import RESULTS_DIR, WORKSPACE_DIR

logger = logging.getLogger(__name__)

def run_healing_agent(repo_url: str, branch_name: str, run_id: str):
    """
    Orchestrator entrypoint:
    1. Prepares workspace
    2. Clones repository
    3. Triggers the LangGraph healing pipeline with progressive updates
    """
    try:
        import git
        from git import Repo
        
        # Force refresh if we have an explicit path set in env
        git_exe = os.environ.get("GIT_PYTHON_GIT_EXECUTABLE")
        if git_exe:
            try:
                git.refresh(git_exe)
            except Exception:
                pass

        from backend.orchestrator.graph import run_healing_pipeline
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Environmental Error: Required dependencies (git/graph) not available: {error_msg}")
        
        # NEAT ERROR CLEANUP: Strip technical GitPython noise
        if "Bad git executable" in error_msg:
             error_msg = "Critical Error: Engine failed to find the 'git' binary on this server."
        
        # Build a more helpful instruction for the user
        advise = "Vercel serverless functions do not support the 'git' binary. Please ensure GITHUB_TOKEN is set to trigger Cloud Healing (GHA)." if os.environ.get("VERCEL") else "Ensure Git is installed on your server and added to your system PATH."
        
        from backend.orchestrator.main import _write_failure
        _write_failure(repo_url, branch_name, run_id, f"{error_msg} {advise}")
        return

    logger.info(f"Starting agent run {run_id} for {repo_url} on {branch_name}")
    
    results_dir = RESULTS_DIR
    os.makedirs(results_dir, exist_ok=True)
    
    repo_dir = os.path.join(WORKSPACE_DIR, run_id)
    
    try:
        if os.path.exists(repo_dir):
            shutil.rmtree(repo_dir)
        os.makedirs(repo_dir, exist_ok=True)
        
        # We pass a callback to run_healing_pipeline to update the JSON progressively
        def update_callback(state: AgentState):
            _write_results(state)

        # 1. Create initial state with MISSION_INITIALIZED event
        from backend.utils.models import CITimelineEvent
        initial_state = AgentState(
            run_id=run_id,
            repo_url=repo_url,
            repo_path=repo_dir,
            branch_name=branch_name,
            start_time=time.time(),
            timeline=[
                CITimelineEvent(
                    iteration=0,
                    event_type="INITIALIZATION",
                    description=f"Mission initialized: Targeting {repo_url}"
                )
            ]
        )
        _write_results(initial_state)

        # 2. Start Cloning
        logger.info(f"Cloning {repo_url} to {repo_dir}...")
        initial_state.timeline.append(CITimelineEvent(
            iteration=0,
            event_type="CLONING",
            description=f"Cloning repository into isolated workspace..."
        ))
        _write_results(initial_state)
        
        if repo_url.startswith("/") or repo_url.startswith("file://"):
            source_path = repo_url.replace("file://", "")
            if os.path.exists(source_path):
                 shutil.copytree(source_path, repo_dir, dirs_exist_ok=True)
                 repo = Repo.init(repo_dir)
                 repo.git.add(A=True)
                 repo.index.commit("Initial local commit")
            else:
                 raise FileNotFoundError(f"Local repo path not found: {source_path}")
        else:
            repo = Repo.clone_from(repo_url, repo_dir)
        
        initial_state.timeline.append(CITimelineEvent(
            iteration=0,
            event_type="CLONING_COMPLETE",
            description=f"Repository clone successful. Launching LangGraph pipeline."
        ))
        _write_results(initial_state)

        final_state = run_healing_pipeline(
            repo_path=repo_dir,
            repo_url=repo_url,
            run_id=run_id,
            branch_name=branch_name,
            on_update=update_callback,
            initial_state=initial_state
        )

        _write_results(final_state)
        logger.info(f"Final results saved for {run_id}")

    except Exception as e:
        logger.error(f"Agent run failed: {e}")
        _write_failure(repo_url, branch_name, run_id, str(e))

import threading
_write_lock = threading.Lock()

def _write_results(state: AgentState):
    """Writes current AgentState to results.json for dashboard consumption."""
    with _write_lock:
        try:
            status_mapped = str(state.ci_status)
            if status_mapped == "SUCCESS":
                status_mapped = "RESOLVED"
            elif status_mapped in ("RUNNING", "CIStatus.RUNNING", "IN_PROGRESS"):
                status_mapped = "IN_PROGRESS"

            scoring_data = {
                "base_score": 100.0, "speed_factor": 0.0, "fix_efficiency": 0.0,
                "regression_penalty": 0.0, "final_ci_score": 0.0
            }
            try:
                if state.scoring:
                    # Map backend model â†’ dashboard contract
                    base = float(getattr(state.scoring, 'base_score', 100.0))
                    speed = float(getattr(state.scoring, 'speed_factor', 0.0))
                    efficiency = float(getattr(state.scoring, 'fix_efficiency', 0.0))
                    penalty = float(getattr(state.scoring, 'regression_penalty', 0.0))
                    total = float(getattr(state.scoring, 'total_score', 0.0))
                    scoring_data = {
                        "base_score": base,
                        "speed_factor": speed,
                        "fix_efficiency": efficiency,
                        "regression_penalty": penalty,
                        "final_ci_score": total
                    }
            except Exception as sc_err:
                logger.error(f"Scoring mapping error: {sc_err}")

            fixes_data = []
            if state.fixes:
                for fix in state.fixes:
                    try:
                        rel_path = fix.file_path
                        repo_path_str = str(state.repo_path)
                        if rel_path.startswith(repo_path_str):
                            rel_path = os.path.relpath(rel_path, repo_path_str)
                        
                        err_type = str(fix.failure_type).split('.')[-1]
                        
                        fixes_data.append({
                            "file_path": rel_path,
                            "error_type": err_type,
                            "original_snippet": fix.original_code,
                            "patched_snippet": fix.patched_code,
                            "tests_passed": fix.validated,
                            "line_number": getattr(fix, 'line_number', None),
                            "commit_message": f"[AI-AGENT] Fix {err_type} in {os.path.basename(rel_path)}"
                        })
                    except Exception as fix_err:
                        logger.error(f"Error mapping fix data: {fix_err}")

            timeline_strings = []
            if hasattr(state, 'timeline') and state.timeline:
                for event in state.timeline:
                    timeline_strings.append(str(event.description))

            # Compute elapsed time
            start_t = getattr(state, 'start_time', None) or time.time()
            elapsed = round(time.time() - start_t, 1)

            # Parse team/leader
            branch_parts = state.branch_name.replace("_AI_FIX", "").split('_')
            mid = len(branch_parts) // 2
            team_name = "_".join(branch_parts[:mid]) if len(branch_parts) > 1 else branch_parts[0]
            leader_name = "_".join(branch_parts[mid:]) if len(branch_parts) > 1 else ""

            result_data = {
                "repo_url": state.repo_url,
                "branch_name": state.branch_name,
                "run_id": state.run_id,
                "total_failures": len(state.failures) if state.failures else 0,
                "total_fixes": len(fixes_data),
                "ci_status": status_mapped,
                "fixes": fixes_data,
                "ci_timeline": timeline_strings,
                "scoring": scoring_data,
                "iterations_used": getattr(state, 'iteration', 0),
                "max_retries": getattr(state, 'max_retries', 5),
                "start_time": start_t,
                "elapsed_seconds": elapsed,
                "team_name": team_name,
                "leader_name": leader_name
            }

            result_file = os.path.join(RESULTS_DIR, f"{state.run_id}.json")
            with open(result_file, 'w') as f:
                json.dump(result_data, f, indent=2)
        except Exception as e:
            logger.error(f"CRITICAL: Failed to write results: {e}\n{traceback.format_exc()}")

def _write_failure(repo_url: str, branch_name: str, run_id: str, error_msg: str):
    result_file = os.path.join(RESULTS_DIR, f"{run_id}.json")
    failure_data = {
        "repo_url": repo_url,
        "branch_name": branch_name,
        "run_id": run_id,
        "total_failures": 0,
        "total_fixes": 0,
        "ci_status": "FAILED",
        "fixes": [],
        "ci_timeline": [f"Critical Error: {error_msg}"],
        "scoring": {
            "base_score": 0, "speed_factor": 0, "fix_efficiency": 0, 
            "regression_penalty": 0, "final_ci_score": 0
        }
    }
    with open(result_file, 'w') as f:
        json.dump(failure_data, f, indent=2)
